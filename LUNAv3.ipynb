{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00001-95841569-0783-4946-95fc-911db545dd64",
    "deepnote_cell_type": "code",
    "execution_millis": 1798,
    "execution_start": 1606948460930,
    "output_cleared": false,
    "source_hash": "69f8c1eb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import standard libraries\n",
    "#!pip install autograd\n",
    "from autograd import numpy as np\n",
    "from autograd import grad\n",
    "from autograd.misc.optimizers import adam, sgd\n",
    "from autograd import scipy as sp\n",
    "import autograd.numpy.random as npr\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "# import our libraries\n",
    "import bayes_helpers as bh\n",
    "from utils import generate_data\n",
    "from utils import run_toy_nn\n",
    "from feed_forward import Feedforward\n",
    "from nlm import NLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00002-25040b6c-3ec6-4bfe-898f-b78f218a9137",
    "deepnote_cell_type": "code",
    "execution_millis": 0,
    "execution_start": 1607010412550,
    "output_cleared": false,
    "source_hash": "945879c6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nlm import NLM\n",
    "from scipy.optimize import approx_fprime\n",
    "class LUNA(NLM):\n",
    "    \"\"\"\n",
    "    Fits LUNA Model; inherits from NLM and overrides the objective function\n",
    "    \n",
    "    Model Assumptions\n",
    "     - Weights distributed normally\n",
    "     - Ys distributed normally\n",
    "\n",
    "     How to use:\n",
    "      - run train() to create: \n",
    "            a) the NN MLE weights, found in self.ff.weights \n",
    "            b) self.posterior samples, the distribution for the weights in the last layer of NLM\n",
    "     \n",
    "      - run predict() to get distribution of ys, given x test\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, prior_var, y_noise_var, architecture, random, grad_func = None):\n",
    "        '''\n",
    "        Important attributes:\n",
    "        \n",
    "        All the NLM attributes\n",
    "\n",
    "        NN attributes:\n",
    "            self.D = not exactly sure...\n",
    "            self.D_in = dimensionality of input data\n",
    "            self.D_out = dimensionality of output data\n",
    "            self.H = width of a layer in the NN\n",
    "        '''\n",
    "\n",
    "        #inherit from NLM, override objective func\n",
    "        super().__init__(prior_var, y_noise_var, architecture, random, self.make_objective) \n",
    "        self.D, self.D_in, self.D_out, self.H = self.ff.D, architecture['input_dim'], architecture['output_dim'], architecture['width']\n",
    "\n",
    "        # override default finite difference method for cosine similarity calc (see cos_sim_sq function)\n",
    "        if grad_func:\n",
    "            self.grad_func = grad_func\n",
    "        else:\n",
    "            self.grad_func = self.default_finite_diff\n",
    "                \n",
    "    def similarity_score(self, W, x):\n",
    "        '''\n",
    "        Calculates total sum of squared cosine similarity between all pairwise combinations of aux \n",
    "        functions\n",
    "        \n",
    "        Inputs: \n",
    "        - W = NumPy array of weights [dim=(1, width H, input dimension D_in)]\n",
    "\n",
    "        Returns:\n",
    "        - score = total cosine similarity squared across all pairs of functions [scalar]\n",
    "\n",
    "        ''' \n",
    "\n",
    "        #D_out = self.D_out\n",
    "        score = 0\n",
    "\n",
    "        #derivs of all the aux funcs\n",
    "        holy_grail = self.grad_func(W, x)\n",
    "\n",
    "        # in dim x out dim x # obs\n",
    "        M = holy_grail.shape[1]\n",
    "        for i in range(self.D_out):\n",
    "            grad_i = holy_grail[:,i,:]\n",
    "            for j in range(i + 1, self.D_out):\n",
    "                grad_j = holy_grail[:,j,:]\n",
    "                score += self.cos_sim_sq(grad_i, grad_j)\n",
    "        return score\n",
    "\n",
    "    def cos_sim_sq(self,grad_i, grad_j):\n",
    "        numer = np.dot(grad_i, grad_j.T)\n",
    "        denom = (np.dot(grad_i,grad_i.T)*np.dot(grad_j,grad_j.T))\n",
    "        return (numer/denom)[0][0]\n",
    "\n",
    "    \n",
    "    def default_finite_diff(self,W,x):\n",
    "        '''\n",
    "        x.shape[0] is # of dimensions == self.D_in\n",
    "        x.shape[1] is # of observations\n",
    "\n",
    "        output: Returns a 3d matrix:\n",
    "                (in dimension) x (out dimension (# of aux functions)) x (# observations)\n",
    "\n",
    "                i.e. for each auxillary function and for each observation, approximate the gradient with dimension x.shape[0]\n",
    "        '''\n",
    "        \n",
    "        #create one epsilon for each observation\n",
    "        eps = np.random.normal(0,0.1,size=x.shape[1])\n",
    "\n",
    "        #iterate over features of raw input data (rows of x)\n",
    "        out = np.zeros((self.D_in, self.D_out, x.shape[1]))\n",
    "\n",
    "        #evaluate function at x\n",
    "        f_ex = self.ff.forward(W, x)\n",
    "\n",
    "        assert x.shape[0] == self.D_in\n",
    "\n",
    "        #for one dimension at a time\n",
    "        for i in range(x.shape[0]):\n",
    "\n",
    "            delta = np.zeros(x.shape)\n",
    "            delta[i,:] = eps\n",
    "\n",
    "            f_eps = self.ff.forward(W,x+delta)\n",
    "\n",
    "            # out dim X #obs\n",
    "            res = (f_eps - f_ex)/eps\n",
    "            #out[i,:,:] = res[0] # value wise division, different epsilon for each column\n",
    "            # NEED TO FIX FOR MULTIDIMENSIONAL INPUT DATA\n",
    "\n",
    "        return res\n",
    "        \n",
    "    def mean_mean_sq_error(self, W):\n",
    "        '''\n",
    "        Calculates average mean sq error across each output nodes (=the aux functions)\n",
    "        \n",
    "        Inputs: \n",
    "        - W = NumPy array of all weights [dim=(1, width H, input dimension D_in)]\n",
    "\n",
    "        Returns:\n",
    "        - mean_mse = mean of mean sq error for each aux function [scalar]\n",
    "        ''' \n",
    "        D, D_out, H = self.D, self.D_out, self.H\n",
    "        aux_outputs = self.ff.forward(W, x_train) #shape = (1,10,12)\n",
    "        Y = np.tile(y_train, D_out).reshape(1, D_out, y_train.shape[1])\n",
    "\n",
    "        # calculate squared error for each aux regressor, take mean\n",
    "        mean_mse = np.mean(np.linalg.norm(Y - aux_outputs, axis=1)**2)\n",
    "\n",
    "        return mean_mse\n",
    "\n",
    "    def make_objective(self, x_train, y_train, reg_param):\n",
    "        '''\n",
    "        Makes objective function and gradient of obj function \n",
    "        \n",
    "        Inputs: \n",
    "        - x_train = NumPy array of training data [dim=(1, anything)]\n",
    "        - y_train = NumPy array of training data [dim=(1, anything)]\n",
    "        - reg_param = regularization parameter [scalar]\n",
    "\n",
    "        Returns:\n",
    "        - objective = function handle for objective function\n",
    "        - grad(objective) = Autograd gradient of objective function\n",
    "        ''' \n",
    "\n",
    "        def objective(W, t):\n",
    "            '''\n",
    "            Calculates objective function: L_luna(model) = L_fit(model) - L_similarity(model)\n",
    "            L_fit(model) = average mean sq error across all outputs/aux functions\n",
    "            L_similarity(model) = sum of squared cosine similarity across all aux function combinations\n",
    "            \n",
    "            Inputs: \n",
    "            - W = NumPy array of all weights [dim=(1, width H, input dimension D_in)]\n",
    "            - t = necessary for adam solver in Autograd (something about creating a callback)\n",
    "\n",
    "            Returns:\n",
    "            - L_fit - L_sim = function handle for objective function [scalar]\n",
    "            '''\n",
    "            # L_luna(model) = L_fit(model) - L_similarity(model)\n",
    "            reg_param = 0.1 #eventually this will become an input\n",
    "            lambda_ = 0.1 #eventually this will become an input\n",
    "            L_sim = lambda_*self.similarity_score(W, x_train)\n",
    "\n",
    "            regularization_penalty = reg_param*np.linalg.norm(W)**2\n",
    "            mean_mse = self.mean_mean_sq_error(W)\n",
    "            L_fit = mean_mse - regularization_penalty\n",
    "\n",
    "            return L_fit - L_sim\n",
    "\n",
    "        return objective, grad(objective)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00003-4fd29ddc-cffd-4379-a466-a9e1c158260a",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "### Define Hyperameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00004-f00b64a0-0d7d-42ba-9604-582df11de5c1",
    "deepnote_cell_type": "code",
    "execution_millis": 3,
    "execution_start": 1607010414347,
    "output_cleared": false,
    "source_hash": "50e5e35e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "###relu activation\n",
    "activation_fn_type = 'relu'\n",
    "activation_fn = lambda x: np.maximum(np.zeros(x.shape), x)\n",
    "\n",
    "\n",
    "###neural network model design choices\n",
    "width = 8\n",
    "hidden_layers = 2\n",
    "input_dim = 1\n",
    "output_dim = 5 #number of auxiliary functions\n",
    "\n",
    "architecture = {'width': width,\n",
    "               'hidden_layers': hidden_layers,\n",
    "               'input_dim': input_dim,\n",
    "               'output_dim': output_dim,\n",
    "               'activation_fn_type': 'relu',\n",
    "               'activation_fn_params': 'rate=1',\n",
    "               'activation_fn': activation_fn}\n",
    "prior_var, y_noise_var = 3.0, 3.0\n",
    "random = np.random.RandomState(0)\n",
    "luna = LUNA(prior_var, y_noise_var, architecture, random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00005-68311c04-947a-4ff2-8b99-48c3e69ec9f2",
    "deepnote_cell_type": "code",
    "execution_millis": 17949,
    "execution_start": 1607010415514,
    "output_cleared": false,
    "source_hash": "e71b43f5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"HW8_data.csv\")\n",
    "x_train = np.array(df[\"x\"])\n",
    "y_train = np.array(df[\"y\"])\n",
    "x_test = np.linspace(x_train.min()-1,x_train.max()+1,200)\n",
    "\n",
    "x_train = x_train.reshape((1, -1))\n",
    "y_train = y_train.reshape((1, -1))\n",
    "x_test = x_test.reshape((1, -1))\n",
    "\n",
    "# 1500 iterations took 5 minutes\n",
    "\n",
    "# gonna see how 6000 iterations does\n",
    "\n",
    "# computes approx 5 iterations per second\n",
    "params = {'step_size':3e-3, \n",
    "          'max_iteration':1500, \n",
    "          'random_restarts':1,\n",
    "          'optimizer':'adam'}\n",
    "\n",
    "luna.train(x_train, y_train, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00006-2dda0d69-aec9-4d10-ac69-acda6c985c6c",
    "deepnote_cell_type": "code",
    "execution_millis": 379,
    "execution_start": 1607010433470,
    "output_cleared": false,
    "source_hash": "f4c0d2be",
    "tags": []
   },
   "outputs": [],
   "source": [
    "posterior_predictions, posterior_predictive_samples = luna.predict(x_test)\n",
    "plt.scatter(x_train, y_train)\n",
    "for i in range(50):\n",
    "    plt.plot(x_test.flatten(), posterior_predictions[i,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00007-31620ea2-9c05-4d4b-a74a-9b4fd9e7619f",
    "deepnote_cell_type": "code",
    "execution_millis": 179,
    "execution_start": 1607010461825,
    "output_cleared": false,
    "source_hash": "894de826",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predict on the test x-values\n",
    "bh.viz_pp_samples(x_train,y_train,x_test.flatten(),posterior_predictive_samples,'La Luna!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00008-bd0b2e89-1b52-4756-8bbb-03afb90b5f1b",
    "deepnote_cell_type": "code",
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "33298fe9-5c82-4b2a-b4a6-8b8f00fdd090",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
