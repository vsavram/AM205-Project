{"cells":[{"cell_type":"markdown","source":"# AM207 Final Project\n\nPaper: *Learned Uncertainty-Aware (LUNA) Bases for Bayesian Regression using Multi-Headed Auxiliary Networks*","metadata":{"tags":[],"cell_id":"00000-244a20a3-5850-49a8-b3c4-47c8dd90e3f4","output_cleared":false}},{"cell_type":"markdown","source":"207 To do:\n- NLM\n    - figure out why get_bayes_lr_posterior() is breaking\n    - Test NLM\n- LUNA\n    - network currently assumes all layers are same size \n    - creating toy dataset examples\n    - auxiliary functions","metadata":{"tags":[],"cell_id":"00001-60483c71-d139-4275-a2dd-d5cb90d94d7f"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00001-82b15dd3-3681-4233-b273-015b39362a48","output_cleared":false,"source_hash":"de8f455b","execution_millis":4181,"execution_start":1605367204124},"source":"!pip install autograd\nfrom autograd import numpy as np\nfrom autograd import grad\nfrom autograd.misc.optimizers import adam, sgd\nfrom autograd import scipy as sp\nimport autograd.numpy.random as npr\nimport pandas as pd\nimport numpy\nimport matplotlib.pyplot as plt\nimport sys","execution_count":3,"outputs":[{"name":"stdout","text":"Collecting autograd\n  Downloading autograd-1.3.tar.gz (38 kB)\nRequirement already satisfied: numpy>=1.12 in /opt/venv/lib/python3.7/site-packages (from autograd) (1.19.4)\nCollecting future>=0.15.2\n  Downloading future-0.18.2.tar.gz (829 kB)\n\u001b[K     |████████████████████████████████| 829 kB 26.0 MB/s \n\u001b[?25hBuilding wheels for collected packages: autograd, future\n  Building wheel for autograd (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for autograd: filename=autograd-1.3-py3-none-any.whl size=47989 sha256=f0b30f2c1cecebf9ca97fa940f6d4b2845fe09e02ae70b06fd4b5b0a3ac6398b\n  Stored in directory: /home/jovyan/.cache/pip/wheels/ef/32/31/0e87227cd0ca1d99ad51fbe4b54c6fa02afccf7e483d045e04\n  Building wheel for future (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491059 sha256=6698a6caf08f99f4ae8bccc1d45706da84268724c5bd728f018df727a7295b32\n  Stored in directory: /home/jovyan/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\nSuccessfully built autograd future\nInstalling collected packages: future, autograd\nSuccessfully installed autograd-1.3 future-0.18.2\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00002-133afcba-b5c2-49d5-828a-458e46e60d99","output_cleared":false,"source_hash":"4b51ed9d","execution_millis":0,"execution_start":1605367309817},"source":"class Feedforward:\n    def __init__(self, architecture, random=None, weights=None):\n        self.params = {'H': architecture['width'],\n                       'L': architecture['hidden_layers'],\n                       'D_in': architecture['input_dim'],\n                       'D_out': architecture['output_dim'],\n                       'activation_type': architecture['activation_fn_type'],\n                       'activation_params': architecture['activation_fn_params']}\n\n        self.D = (  (architecture['input_dim'] * architecture['width'] + architecture['width'])\n                  + (architecture['output_dim'] * architecture['width'] + architecture['output_dim'])\n                  + (architecture['hidden_layers'] - 1) * (architecture['width']**2 + architecture['width'])\n                 )\n\n        if random is not None:\n            self.random = random\n        else:\n            self.random = np.random.RandomState(0)\n\n        self.h = architecture['activation_fn']\n\n        if weights is None:\n            self.weights = self.random.normal(0, 1, size=(1, self.D))\n        else:\n            self.weights = weights\n\n        self.objective_trace = np.empty((1, 1))\n        self.weight_trace = np.empty((1, self.D))\n\n\n    def forward(self, weights, x, final_layer_out=False):\n        ''' Forward pass given weights and input '''\n        H = self.params['H']\n        D_in = self.params['D_in']\n        D_out = self.params['D_out']\n\n        assert weights.shape[1] == self.D\n\n        if len(x.shape) == 2:\n            assert x.shape[0] == D_in\n            x = x.reshape((1, D_in, -1))\n        else:\n            assert x.shape[1] == D_in\n\n        weights = weights.T\n\n        #input to first hidden layer\n        W = weights[:H * D_in].T.reshape((-1, H, D_in))\n        b = weights[H * D_in:H * D_in + H].T.reshape((-1, H, 1))\n        input = self.h(np.matmul(W, x) + b)\n        index = H * D_in + H\n\n        assert input.shape[1] == H\n\n        #additional hidden layers\n        for _ in range(self.params['L'] - 1):\n            before = index\n            W = weights[index:index + H * H].T.reshape((-1, H, H))\n            index += H * H\n            b = weights[index:index + H].T.reshape((-1, H, 1))\n            index += H\n            output = np.matmul(W, input) + b\n            input = self.h(output)\n\n            assert input.shape[1] == H\n\n        #output layer #THIS IS JUST FOR FINAL LAYER, INPUT STORES VALUES OF FINAL ALYER\n        W = weights[index:index + H * D_out].T.reshape((-1, D_out, H))\n        b = weights[index + H * D_out:].T.reshape((-1, D_out, 1))\n        output = np.matmul(W, input) + b\n        assert output.shape[1] == self.params['D_out']\n\n        #finallayer\n        final_layer = np.array(input, copy=True) #autograd doesn't like np.copy\n\n        if final_layer_out:\n            return final_layer\n        else:\n            return output\n    \n    def make_objective(self, x_train, y_train, reg_param):\n\n        def objective(W, t):\n            squared_error = np.linalg.norm(y_train - self.forward(W, x_train), axis=1)**2\n            if reg_param is None:\n                sum_error = np.sum(squared_error)\n                return sum_error\n            else:\n                mean_error = np.mean(squared_error) + reg_param * np.linalg.norm(W)\n                return mean_error\n\n        return objective, grad(objective)\n\n    def fit(self, x_train, y_train, params, reg_param=None,):\n\n        assert x_train.shape[0] == self.params['D_in']\n        assert y_train.shape[0] == self.params['D_out']\n\n        ### make objective function for training\n        self.objective, self.gradient = self.make_objective(x_train, y_train, reg_param)\n\n        ### set up optimization\n        step_size = 0.01\n        max_iteration = 5000\n        check_point = 100\n        weights_init = self.weights.reshape((1, -1))\n        mass = None\n        optimizer = 'adam' # DEFAULT. CHANGE IN PARAMS\n        opt_gradient = self.gradient # DEFAULT. CHANGE IN PARAMS\n        random_restarts = 5\n\n        if 'step_size' in params.keys():\n            step_size = params['step_size']\n        if 'max_iteration' in params.keys():\n            max_iteration = params['max_iteration']\n        if 'check_point' in params.keys():\n            self.check_point = params['check_point']\n        if 'init' in params.keys():\n            weights_init = params['init']\n        if 'call_back' in params.keys():\n            call_back = params['call_back']\n        if 'mass' in params.keys():\n            mass = params['mass']\n        if 'optimizer' in params.keys():\n            optimizer = params['optimizer']\n        if  'opt_gradient' in params.keys():\n            gradient = params['opt_gradient']\n        if 'random_restarts' in params.keys():\n            random_restarts = params['random_restarts']\n\n        def call_back(weights, iteration, g):\n            ''' Actions per optimization step '''\n            objective = self.objective(weights, iteration)\n            self.objective_trace = np.vstack((self.objective_trace, objective))\n            self.weight_trace = np.vstack((self.weight_trace, weights))\n            if iteration % check_point == 0:\n                print(\"Iteration {} lower bound {}; gradient mag: {}\".format(iteration, objective, np.linalg.norm(self.gradient(weights, iteration))))\n\n        ### train with random restarts\n        optimal_obj = 1e16\n        optimal_weights = self.weights\n\n        # AM205 CREW: gradient is used here\n        for i in range(random_restarts):\n\n            # this is the default\n            if optimizer == 'adam':\n                adam(opt_gradient, weights_init, step_size=step_size, num_iters=max_iteration, callback=call_back)\n            \n            local_opt = np.min(self.objective_trace[-100:])\n            if local_opt < optimal_obj:\n                opt_index = np.argmin(self.objective_trace[-100:])\n                self.weights = self.weight_trace[-100:][opt_index].reshape((1, -1))\n            weights_init = self.random.normal(0, 1, size=(1, self.D))\n\n        self.objective_trace = self.objective_trace[1:]\n        self.weight_trace = self.weight_trace[1:]","execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00002-da662230-83a9-4a83-a071-abb21b920110","output_cleared":false,"source_hash":"2d09fe3d","execution_millis":2,"execution_start":1605367310838},"source":"\ndata = pd.read_csv('HW8_data.csv')\nx_train = data['x'].values.reshape((1, -1))\ny_train = data['y'].values.reshape((1, -1))\ndata","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"application/vnd.deepnote.dataframe.v2+json":{"row_count":12,"column_count":2,"columns":[{"name":"x","dtype":"float64","stats":{"unique_count":12,"nan_count":0,"min":-6,"max":6,"histogram":[{"bin_start":-6,"bin_end":-4.8,"count":3},{"bin_start":-4.8,"bin_end":-3.6,"count":3},{"bin_start":-3.6,"bin_end":-2.4000000000000004,"count":0},{"bin_start":-2.4000000000000004,"bin_end":-1.2000000000000002,"count":0},{"bin_start":-1.2000000000000002,"bin_end":0,"count":0},{"bin_start":0,"bin_end":1.1999999999999993,"count":0},{"bin_start":1.1999999999999993,"bin_end":2.4000000000000004,"count":0},{"bin_start":2.4000000000000004,"bin_end":3.5999999999999996,"count":0},{"bin_start":3.5999999999999996,"bin_end":4.799999999999999,"count":2},{"bin_start":4.799999999999999,"bin_end":6,"count":4}]}},{"name":"y","dtype":"float64","stats":{"unique_count":12,"nan_count":0,"min":-3.3802835594264837,"max":1.8242396746313259,"histogram":[{"bin_start":-3.3802835594264837,"bin_end":-2.8598312360207028,"count":2},{"bin_start":-2.8598312360207028,"bin_end":-2.339378912614922,"count":1},{"bin_start":-2.339378912614922,"bin_end":-1.8189265892091409,"count":1},{"bin_start":-1.8189265892091409,"bin_end":-1.29847426580336,"count":1},{"bin_start":-1.29847426580336,"bin_end":-0.7780219423975789,"count":0},{"bin_start":-0.7780219423975789,"bin_end":-0.257569618991798,"count":1},{"bin_start":-0.257569618991798,"bin_end":0.262882704413983,"count":1},{"bin_start":0.262882704413983,"bin_end":0.7833350278197639,"count":0},{"bin_start":0.7833350278197639,"bin_end":1.303787351225545,"count":0},{"bin_start":1.303787351225545,"bin_end":1.8242396746313259,"count":5}]}},{"name":"_deepnote_index_column","dtype":"int64"}],"rows_top":[{"x":-6,"y":-3.3802835594264837,"_deepnote_index_column":0},{"x":-5.6,"y":-2.892116654207945,"_deepnote_index_column":1},{"x":-5.2,"y":-2.6900586893764498,"_deepnote_index_column":2},{"x":-4.8,"y":-2.039999683466143,"_deepnote_index_column":3},{"x":-4.4,"y":-1.3999416602887405,"_deepnote_index_column":4},{"x":-4,"y":-0.4447009780288869,"_deepnote_index_column":5},{"x":4,"y":0.015153379764518691,"_deepnote_index_column":6},{"x":4.4,"y":1.820702411345962,"_deepnote_index_column":7},{"x":4.8,"y":1.8242396746313259,"_deepnote_index_column":8},{"x":5.2,"y":1.4768319204220486,"_deepnote_index_column":9},{"x":5.6,"y":1.4351581263067057,"_deepnote_index_column":10},{"x":6,"y":1.374900997831896,"_deepnote_index_column":11}],"rows_bottom":null},"text/plain":"      x         y\n0  -6.0 -3.380284\n1  -5.6 -2.892117\n2  -5.2 -2.690059\n3  -4.8 -2.040000\n4  -4.4 -1.399942\n5  -4.0 -0.444701\n6   4.0  0.015153\n7   4.4  1.820702\n8   4.8  1.824240\n9   5.2  1.476832\n10  5.6  1.435158\n11  6.0  1.374901","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-6.0</td>\n      <td>-3.380284</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-5.6</td>\n      <td>-2.892117</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-5.2</td>\n      <td>-2.690059</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-4.8</td>\n      <td>-2.040000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-4.4</td>\n      <td>-1.399942</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>-4.0</td>\n      <td>-0.444701</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>4.0</td>\n      <td>0.015153</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>4.4</td>\n      <td>1.820702</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>4.8</td>\n      <td>1.824240</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>5.2</td>\n      <td>1.476832</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>5.6</td>\n      <td>1.435158</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>6.0</td>\n      <td>1.374901</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00004-6632e33c-4eea-427c-9d17-87c54912c224","output_cleared":false,"source_hash":"aeae959c","execution_millis":2,"execution_start":1605367311984},"source":"###relu activation\nactivation_fn_type = 'relu'\nactivation_fn = lambda x: np.maximum(np.zeros(x.shape), x)\n\n\n###neural network model design choices\nwidth = 5\nhidden_layers = 1\ninput_dim = 1\noutput_dim = 1\n\narchitecture = {'width': width,\n               'hidden_layers': hidden_layers,\n               'input_dim': input_dim,\n               'output_dim': output_dim,\n               'activation_fn_type': 'relu',\n               'activation_fn_params': 'rate=1',\n               'activation_fn': activation_fn}\n\n#set random state to make the experiments replicable\nrand_state = 0\nrandom = np.random.RandomState(rand_state)\n\n#instantiate a Feedforward neural network object\nnn = Feedforward(architecture, random=random)","execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00005-9ecfb438-c35a-48c3-b29e-107f9e825364","output_cleared":false,"source_hash":"f1861af","execution_millis":19351,"execution_start":1605367314015},"source":"###define design choices in gradient descent\nparams = {'step_size':1e-3, \n          'max_iteration':15000, \n          'random_restarts':1,\n          'optimizer':'adam'}\n\n#fit my neural network to minimize MSE on the given data\nnn.fit(x_train, y_train, params)","execution_count":15,"outputs":[{"name":"stdout","text":"Iteration 0 lower bound 1188.252211958758; gradient mag: 3294.94720051455\nIteration 100 lower bound 528.2005162282458; gradient mag: 2014.0223481861012\nIteration 200 lower bound 231.71928757081068; gradient mag: 1226.9831178661013\nIteration 300 lower bound 103.99541756891784; gradient mag: 716.7107003654074\nIteration 400 lower bound 55.24716113156887; gradient mag: 376.2334633599828\nIteration 500 lower bound 39.94989749994618; gradient mag: 177.9641773056736\nIteration 600 lower bound 35.60307206137053; gradient mag: 77.19358484414683\nIteration 700 lower bound 34.02585610754244; gradient mag: 38.67687513287846\nIteration 800 lower bound 32.9614293614884; gradient mag: 31.129490339137096\nIteration 900 lower bound 31.989641942996613; gradient mag: 24.752496721932584\nIteration 1000 lower bound 31.082564536611727; gradient mag: 24.616299563320247\nIteration 1100 lower bound 30.155032233758106; gradient mag: 24.248289796695587\nIteration 1200 lower bound 29.210013809597285; gradient mag: 23.699589428153832\nIteration 1300 lower bound 28.25300661469738; gradient mag: 23.095433479427726\nIteration 1400 lower bound 27.28927363692245; gradient mag: 22.475317737860436\nIteration 1500 lower bound 26.323662523356894; gradient mag: 21.850371675396286\nIteration 1600 lower bound 25.360599282998077; gradient mag: 21.22512682625579\nIteration 1700 lower bound 24.40409028689416; gradient mag: 20.60271393789144\nIteration 1800 lower bound 23.457725796495904; gradient mag: 19.985836957824535\nIteration 1900 lower bound 22.524686048678994; gradient mag: 19.376902849934968\nIteration 2000 lower bound 21.607750701236885; gradient mag: 18.778013849409053\nIteration 2100 lower bound 20.70931205242157; gradient mag: 18.190943350296994\nIteration 2200 lower bound 19.831392143669202; gradient mag: 17.61711368632843\nIteration 2300 lower bound 18.9756636132038; gradient mag: 17.05757941374588\nIteration 2400 lower bound 18.143473974355437; gradient mag: 16.51301792143842\nIteration 2500 lower bound 17.33587283812604; gradient mag: 15.983728786591476\nIteration 2600 lower bound 16.553641480811155; gradient mag: 15.469642874398215\nIteration 2700 lower bound 15.79979845292681; gradient mag: 14.63410021670294\nIteration 2800 lower bound 15.089224741859162; gradient mag: 13.799477500655174\nIteration 2900 lower bound 14.412878302812285; gradient mag: 13.252437242012407\nIteration 3000 lower bound 13.766721311122604; gradient mag: 12.795482048078641\nIteration 3100 lower bound 13.147000834095964; gradient mag: 12.39916575801765\nIteration 3200 lower bound 12.550715085203827; gradient mag: 12.043477234157674\nIteration 3300 lower bound 11.987988709794728; gradient mag: 11.671698244924366\nIteration 3400 lower bound 11.470331425797703; gradient mag: 11.552845486196434\nIteration 3500 lower bound 10.989896461078452; gradient mag: 11.57014517035335\nIteration 3600 lower bound 10.545865383076633; gradient mag: 9.157301353805469\nIteration 3700 lower bound 10.134390330913078; gradient mag: 8.739366437372633\nIteration 3800 lower bound 9.744699797627586; gradient mag: 8.48450205329216\nIteration 3900 lower bound 9.369437622842208; gradient mag: 8.279364759162563\nIteration 4000 lower bound 9.023880352484412; gradient mag: 12.814088131372912\nIteration 4100 lower bound 8.714826491071893; gradient mag: 8.636545905932062\nIteration 4200 lower bound 8.436787581496315; gradient mag: 9.251058145534492\nIteration 4300 lower bound 8.187373568381846; gradient mag: 7.442042690088613\nIteration 4400 lower bound 7.964500968027264; gradient mag: 6.053454321911013\nIteration 4500 lower bound 7.766266930187458; gradient mag: 5.051211535334474\nIteration 4600 lower bound 7.589279565032135; gradient mag: 4.597739322147367\nIteration 4700 lower bound 7.42241063455821; gradient mag: 4.456507453357347\nIteration 4800 lower bound 7.258760999455524; gradient mag: 4.341240693065324\nIteration 4900 lower bound 7.106644631620015; gradient mag: 4.521272792643243\nIteration 5000 lower bound 6.974070400942563; gradient mag: 5.201564339555993\nIteration 5100 lower bound 6.8569725612895835; gradient mag: 6.119331677603754\nIteration 5200 lower bound 6.7529199814522345; gradient mag: 7.133712145135426\nIteration 5300 lower bound 6.660138579044938; gradient mag: 8.182158552640512\nIteration 5400 lower bound 6.5758795412477244; gradient mag: 9.166712555050196\nIteration 5500 lower bound 6.498596062906509; gradient mag: 3.066303145782268\nIteration 5600 lower bound 6.426834605421496; gradient mag: 2.43313811355471\nIteration 5700 lower bound 6.357433884941597; gradient mag: 2.2603654201225614\nIteration 5800 lower bound 6.286273937604809; gradient mag: 2.2395351518992443\nIteration 5900 lower bound 6.213043834805277; gradient mag: 2.2258492140245365\nIteration 6000 lower bound 6.137748546533555; gradient mag: 2.2122700356057496\nIteration 6100 lower bound 6.060402578608477; gradient mag: 2.198351168395081\nIteration 6200 lower bound 5.982027488105806; gradient mag: 2.133700892111169\nIteration 6300 lower bound 5.905113988567323; gradient mag: 2.432126797813228\nIteration 6400 lower bound 5.82831912957614; gradient mag: 2.8272278412164757\nIteration 6500 lower bound 5.75183063954626; gradient mag: 3.3968392180464484\nIteration 6600 lower bound 5.672819183849377; gradient mag: 3.5111721064563857\nIteration 6700 lower bound 5.593319432198851; gradient mag: 3.936185498220979\nIteration 6800 lower bound 5.5117703864218; gradient mag: 4.246984794757411\nIteration 6900 lower bound 5.428607645781736; gradient mag: 14.014120586219276\nIteration 7000 lower bound 5.344999645585855; gradient mag: 4.847156007867576\nIteration 7100 lower bound 5.2587731740380494; gradient mag: 5.037990425758354\nIteration 7200 lower bound 5.1702455486511285; gradient mag: 4.913896127365952\nIteration 7300 lower bound 5.081990798375703; gradient mag: 5.148618362507478\nIteration 7400 lower bound 4.992323968370186; gradient mag: 5.062612851660922\nIteration 7500 lower bound 4.902196879553127; gradient mag: 5.07823330561524\nIteration 7600 lower bound 4.81169107673321; gradient mag: 12.726071077503883\nIteration 7700 lower bound 4.722034252465465; gradient mag: 5.627923598935906\nIteration 7800 lower bound 4.6304661552408675; gradient mag: 5.396531979077218\nIteration 7900 lower bound 4.539356504132501; gradient mag: 5.2116672953127505\nIteration 8000 lower bound 4.45095546939241; gradient mag: 5.442647223211044\nIteration 8100 lower bound 4.3600620974975275; gradient mag: 5.203891398270185\nIteration 8200 lower bound 4.271237398872255; gradient mag: 5.050050070322315\nIteration 8300 lower bound 4.184440979576095; gradient mag: 5.246792589389047\nIteration 8400 lower bound 4.098659993997209; gradient mag: 5.578840619974407\nIteration 8500 lower bound 4.0122914943253445; gradient mag: 5.38143747376593\nIteration 8600 lower bound 3.9284516457036736; gradient mag: 5.346155288618987\nIteration 8700 lower bound 3.843939536519582; gradient mag: 9.992393975865559\nIteration 8800 lower bound 3.762354169166514; gradient mag: 9.79791476897377\nIteration 8900 lower bound 3.680935626235936; gradient mag: 9.23395114566231\nIteration 9000 lower bound 3.6026984128920043; gradient mag: 9.590713589033799\nIteration 9100 lower bound 3.522741405374154; gradient mag: 4.8315358324128885\nIteration 9200 lower bound 3.4454461281570974; gradient mag: 4.546978550730172\nIteration 9300 lower bound 3.3682333282315446; gradient mag: 8.077544206848168\nIteration 9400 lower bound 3.2931617717908868; gradient mag: 5.198762473040716\nIteration 9500 lower bound 3.215814184041519; gradient mag: 3.9061861443066372\nIteration 9600 lower bound 3.1406962333053876; gradient mag: 3.8992430901073822\nIteration 9700 lower bound 3.065866396957153; gradient mag: 6.9120899421115425\nIteration 9800 lower bound 2.990824541196763; gradient mag: 3.9861290594190173\nIteration 9900 lower bound 2.916690346335627; gradient mag: 6.789802894149524\nIteration 10000 lower bound 2.8425619783889506; gradient mag: 3.277143244727384\nIteration 10100 lower bound 2.7688158351972345; gradient mag: 2.6436406206321457\nIteration 10200 lower bound 2.6967620063896316; gradient mag: 1.7554182380350003\nIteration 10300 lower bound 2.626726890192881; gradient mag: 1.9079914587415372\nIteration 10400 lower bound 2.5588304285321404; gradient mag: 1.0932264174021213\nIteration 10500 lower bound 2.4941680185437414; gradient mag: 0.7730587380366593\nIteration 10600 lower bound 2.4333164901781315; gradient mag: 0.767066193236934\nIteration 10700 lower bound 2.376555349336709; gradient mag: 0.7553579421583788\nIteration 10800 lower bound 2.3238459422563444; gradient mag: 0.7228024683427984\nIteration 10900 lower bound 2.2753308343301666; gradient mag: 0.6845211470547401\nIteration 11000 lower bound 2.2312496509944224; gradient mag: 0.6413826587051727\nIteration 11100 lower bound 2.1918278044347352; gradient mag: 0.5943225261136371\nIteration 11200 lower bound 2.1572058127250573; gradient mag: 0.5444130874953347\nIteration 11300 lower bound 2.1273989093957475; gradient mag: 0.49279021686797564\nIteration 11400 lower bound 2.102281353226895; gradient mag: 0.4406015166810731\nIteration 11500 lower bound 2.0815910694099817; gradient mag: 0.38895956298853407\nIteration 11600 lower bound 2.064950099998059; gradient mag: 0.3388961083073893\nIteration 11700 lower bound 2.0518957709682164; gradient mag: 0.29131924913191776\nIteration 11800 lower bound 2.0419171399915657; gradient mag: 0.24697771788261189\nIteration 11900 lower bound 2.034491504029226; gradient mag: 0.206436219130497\nIteration 12000 lower bound 2.029116587353764; gradient mag: 0.1700642338625467\nIteration 12100 lower bound 2.025335369827164; gradient mag: 0.13803877718611637\nIteration 12200 lower bound 2.0227520966449344; gradient mag: 0.11035976785039378\nIteration 12300 lower bound 2.0210395323013945; gradient mag: 0.08687528646323558\nIteration 12400 lower bound 2.019938719988482; gradient mag: 0.06731321537522404\nIteration 12500 lower bound 2.0192532256688813; gradient mag: 0.051315577914184445\nIteration 12600 lower bound 2.0188400594620646; gradient mag: 0.03847223700586815\nIteration 12700 lower bound 2.018599266629263; gradient mag: 0.02835131706866659\nIteration 12800 lower bound 2.0184637229425713; gradient mag: 0.02052459535487594\nIteration 12900 lower bound 2.0183901201436676; gradient mag: 0.014586998208491776\nIteration 13000 lower bound 2.018351617807739; gradient mag: 0.010170104881023347\nIteration 13100 lower bound 2.0183322452743613; gradient mag: 0.00695013437505628\nIteration 13200 lower bound 2.0183228858875397; gradient mag: 0.004651251256489226\nIteration 13300 lower bound 2.0183185522950673; gradient mag: 0.0030451959052784556\nIteration 13400 lower bound 2.0183166332413283; gradient mag: 0.0019482655844909973\nIteration 13500 lower bound 2.0183158223137183; gradient mag: 0.0012165923762946223\nIteration 13600 lower bound 2.01831549612612; gradient mag: 0.0007405247328146874\nIteration 13700 lower bound 2.0183153715620206; gradient mag: 0.0004387539424643723\nIteration 13800 lower bound 2.01831532653006; gradient mag: 0.00025265818503618305\nIteration 13900 lower bound 2.0183153111653804; gradient mag: 0.00014117995468807326\nIteration 14000 lower bound 2.01831530623388; gradient mag: 7.641641264450355e-05\nIteration 14100 lower bound 2.0183153047500992; gradient mag: 3.999153140609237e-05\nIteration 14200 lower bound 2.0183153043331785; gradient mag: 2.0195504443660263e-05\nIteration 14300 lower bound 2.018315304224201; gradient mag: 9.820275317886484e-06\nIteration 14400 lower bound 2.018315304197827; gradient mag: 4.5876100911339915e-06\nIteration 14500 lower bound 2.0183153041919355; gradient mag: 2.0539213644144947e-06\nIteration 14600 lower bound 2.0183153041907285; gradient mag: 8.78979081776736e-07\nIteration 14700 lower bound 2.0183153041905064; gradient mag: 3.585482184541415e-07\nIteration 14800 lower bound 2.0183356471124654; gradient mag: 0.6225847787417013\nIteration 14900 lower bound 2.0183153050798386; gradient mag: 0.004103000324762999\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00006-636ff1b9-fa81-43a9-aeaa-6ca733538a52","output_cleared":false,"source_hash":"febdf14f","execution_millis":129,"execution_start":1605302323264},"source":"#test x-values\nx_test = np.linspace(-8, 8, 100).reshape((1, -1))\nprint(nn.weights.shape, x_test.shape)\n#predict on the test x-values\ny_test_pred = nn.forward(nn.weights, x_test)\n#visualize the function learned by the neural network\nplt.scatter(x_train.flatten(), y_train.flatten(), color='black', label='data')\nplt.plot(x_test.flatten(), y_test_pred.flatten(), color='red', label='learned neural network function')\nplt.legend(loc='best')\nplt.show()","execution_count":null,"outputs":[{"name":"stdout","text":"(1, 16) (1, 100)\n","output_type":"stream"},{"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"},"output_type":"display_data"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00000-2aed2219-ce40-41ba-9030-c82365f32b30","output_cleared":false,"source_hash":"f3158ead","execution_millis":1,"execution_start":1605303617152},"source":"##\nfrom scipy.stats import multivariate_normal\n\nclass NLM():\n\n    def __init__(self,X,Y,prior_var,y_var,architecture, random_state):\n\n        self.ff = Feedforward(architecture, random = random_state) #CHANGE\n        \n        #super().__init__(architecture)\n        self.X = X # X training\n        self.Y = Y # Y training data\n        \n        self.prior_var = prior_var\n        self.y_var = y_var\n        self.w_prior_pdf = lambda w : multivariate_normal(mean = np.zeros(shape = w.shape), cov = prior_variance*np.eye(w.shape[1])).pdf(w)\n        self.w_prior_sampler = lambda : np.random.default_rng().multivariate_normal(mean = np.zeros(shape = w.shape), cov = prior_variance*np.eye(w.shape[1]))\n\n    def train(self):\n        \n        # Fit Weights\n        self.ff.fit(self.X, self.Y, params)\n\n        # Transform X with Feature Map for Bayes Reg\n        fm_x_matrix = self.ff.forward(self.ff.weights, self.X, final_layer_out=True)\n        \n        print(fm_x_matrix)\n        # Conduct Bayes Reg on Final Layer \n        posterior_samples = self.get_bayes_lr_posterior(self.prior_var,\n                                                                self.y_var,\n                                                                fm_x_matrix.T, #fm_x_matrix, \n                                                                self.Y.T, \n                                                                x_test_matrix = None, \n                                                                samples=100)\n\n\n        print(\"DONE\")\n        \n        \n\n    \n    def get_bayes_lr_posterior(self, prior_var, noise_var, x_matrix, y_matrix, x_test_matrix=None, samples=100):\n        '''Function to generate posterior predictive samples for Bayesian linear regression model'''\n        prior_variance = np.diag(prior_var * np.ones(x_matrix.shape[1])) # make it 2 D\n        prior_precision = np.linalg.inv(prior_variance)\n\n        joint_precision = prior_precision + x_matrix.T.dot(x_matrix) / noise_var \n        joint_variance = np.linalg.inv(joint_precision)\n        joint_mean = joint_variance.dot(x_matrix.T.dot(y_matrix)) / noise_var\n\n        #sampling 100 points from the posterior\n        posterior_samples = np.random.multivariate_normal(joint_mean.flatten(), joint_variance, size=samples)\n\n        #take posterior predictive samples\n        if x_test_matrix != None:\n            posterior_predictions = np.dot(posterior_samples, x_test_matrix.T) \n            posterior_predictive_samples = posterior_predictions[np.newaxis, :, :] + np.random.normal(0, noise_var**0.5, size=(100, posterior_predictions.shape[0], posterior_predictions.shape[1]))\n            posterior_predictive_samples = posterior_predictive_samples.reshape((100 * posterior_predictions.shape[0], posterior_predictions.shape[1]))\n            return posterior_predictions, posterior_predictive_samples\n        else:\n            return posterior_samples","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00000-f0eaac6f-fe09-47ff-8d9b-6b57b7ca5d3e","output_cleared":false,"source_hash":"d9f4c07","execution_millis":3,"execution_start":1605303618273},"source":"\nprior_var = 1.0\ny_var = 2.0\ntest_nlm = NLM(x_train, y_train, prior_var,y_var, architecture, random_state = np.random.RandomState(0))","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00005-3b5e8825-04a1-4a79-80e0-44a0bbabf7d5","output_cleared":false,"source_hash":"476b857b","execution_millis":19859,"execution_start":1605303618618},"source":"test_nlm.train()","execution_count":null,"outputs":[{"name":"stdout","text":"Iteration 0 lower bound 1188.252211958758; gradient mag: 3294.94720051455\nIteration 100 lower bound 528.2005162282458; gradient mag: 2014.0223481861012\nIteration 200 lower bound 231.71928757081068; gradient mag: 1226.9831178661013\nIteration 300 lower bound 103.99541756891784; gradient mag: 716.7107003654074\nIteration 400 lower bound 55.24716113156887; gradient mag: 376.2334633599828\nIteration 500 lower bound 39.94989749994618; gradient mag: 177.9641773056736\nIteration 600 lower bound 35.60307206137053; gradient mag: 77.19358484414683\nIteration 700 lower bound 34.02585610754244; gradient mag: 38.67687513287846\nIteration 800 lower bound 32.9614293614884; gradient mag: 31.129490339137096\nIteration 900 lower bound 31.989641942996613; gradient mag: 24.752496721932584\nIteration 1000 lower bound 31.082564536611727; gradient mag: 24.616299563320247\nIteration 1100 lower bound 30.155032233758106; gradient mag: 24.248289796695587\nIteration 1200 lower bound 29.210013809597285; gradient mag: 23.699589428153832\nIteration 1300 lower bound 28.25300661469738; gradient mag: 23.095433479427726\nIteration 1400 lower bound 27.28927363692245; gradient mag: 22.475317737860436\nIteration 1500 lower bound 26.323662523356894; gradient mag: 21.850371675396286\nIteration 1600 lower bound 25.360599282998077; gradient mag: 21.22512682625579\nIteration 1700 lower bound 24.40409028689416; gradient mag: 20.60271393789144\nIteration 1800 lower bound 23.457725796495904; gradient mag: 19.985836957824535\nIteration 1900 lower bound 22.524686048678994; gradient mag: 19.376902849934968\nIteration 2000 lower bound 21.607750701236885; gradient mag: 18.778013849409053\nIteration 2100 lower bound 20.70931205242157; gradient mag: 18.190943350296994\nIteration 2200 lower bound 19.831392143669202; gradient mag: 17.61711368632843\nIteration 2300 lower bound 18.9756636132038; gradient mag: 17.05757941374588\nIteration 2400 lower bound 18.143473974355437; gradient mag: 16.51301792143842\nIteration 2500 lower bound 17.33587283812604; gradient mag: 15.983728786591476\nIteration 2600 lower bound 16.553641480811155; gradient mag: 15.469642874398215\nIteration 2700 lower bound 15.79979845292681; gradient mag: 14.63410021670294\nIteration 2800 lower bound 15.089224741859162; gradient mag: 13.799477500655174\nIteration 2900 lower bound 14.412878302812285; gradient mag: 13.252437242012407\nIteration 3000 lower bound 13.766721311122604; gradient mag: 12.795482048078641\nIteration 3100 lower bound 13.147000834095964; gradient mag: 12.39916575801765\nIteration 3200 lower bound 12.550715085203827; gradient mag: 12.043477234157674\nIteration 3300 lower bound 11.987988709794728; gradient mag: 11.671698244924366\nIteration 3400 lower bound 11.470331425797703; gradient mag: 11.552845486196434\nIteration 3500 lower bound 10.989896461078452; gradient mag: 11.57014517035335\nIteration 3600 lower bound 10.545865383076633; gradient mag: 9.157301353805469\nIteration 3700 lower bound 10.134390330913078; gradient mag: 8.739366437372633\nIteration 3800 lower bound 9.744699797627586; gradient mag: 8.48450205329216\nIteration 3900 lower bound 9.369437622842208; gradient mag: 8.279364759162563\nIteration 4000 lower bound 9.023880352484412; gradient mag: 12.814088131372912\nIteration 4100 lower bound 8.714826491071893; gradient mag: 8.636545905932062\nIteration 4200 lower bound 8.436787581496315; gradient mag: 9.251058145534492\nIteration 4300 lower bound 8.187373568381846; gradient mag: 7.442042690088613\nIteration 4400 lower bound 7.964500968027264; gradient mag: 6.053454321911013\nIteration 4500 lower bound 7.766266930187458; gradient mag: 5.051211535334474\nIteration 4600 lower bound 7.589279565032135; gradient mag: 4.597739322147367\nIteration 4700 lower bound 7.42241063455821; gradient mag: 4.456507453357347\nIteration 4800 lower bound 7.258760999455524; gradient mag: 4.341240693065324\nIteration 4900 lower bound 7.106644631620015; gradient mag: 4.521272792643243\nIteration 5000 lower bound 6.974070400942563; gradient mag: 5.201564339555993\nIteration 5100 lower bound 6.8569725612895835; gradient mag: 6.119331677603754\nIteration 5200 lower bound 6.7529199814522345; gradient mag: 7.133712145135426\nIteration 5300 lower bound 6.660138579044938; gradient mag: 8.182158552640512\nIteration 5400 lower bound 6.5758795412477244; gradient mag: 9.166712555050196\nIteration 5500 lower bound 6.498596062906509; gradient mag: 3.066303145782268\nIteration 5600 lower bound 6.426834605421496; gradient mag: 2.43313811355471\nIteration 5700 lower bound 6.357433884941597; gradient mag: 2.2603654201225614\nIteration 5800 lower bound 6.286273937604809; gradient mag: 2.2395351518992443\nIteration 5900 lower bound 6.213043834805277; gradient mag: 2.2258492140245365\nIteration 6000 lower bound 6.137748546533555; gradient mag: 2.2122700356057496\nIteration 6100 lower bound 6.060402578608477; gradient mag: 2.198351168395081\nIteration 6200 lower bound 5.982027488105806; gradient mag: 2.133700892111169\nIteration 6300 lower bound 5.905113988567323; gradient mag: 2.432126797813228\nIteration 6400 lower bound 5.82831912957614; gradient mag: 2.8272278412164757\nIteration 6500 lower bound 5.75183063954626; gradient mag: 3.3968392180464484\nIteration 6600 lower bound 5.672819183849377; gradient mag: 3.5111721064563857\nIteration 6700 lower bound 5.593319432198851; gradient mag: 3.936185498220979\nIteration 6800 lower bound 5.5117703864218; gradient mag: 4.246984794757411\nIteration 6900 lower bound 5.428607645781736; gradient mag: 14.014120586219276\nIteration 7000 lower bound 5.344999645585855; gradient mag: 4.847156007867576\nIteration 7100 lower bound 5.2587731740380494; gradient mag: 5.037990425758354\nIteration 7200 lower bound 5.1702455486511285; gradient mag: 4.913896127365952\nIteration 7300 lower bound 5.081990798375703; gradient mag: 5.148618362507478\nIteration 7400 lower bound 4.992323968370186; gradient mag: 5.062612851660922\nIteration 7500 lower bound 4.902196879553127; gradient mag: 5.07823330561524\nIteration 7600 lower bound 4.81169107673321; gradient mag: 12.726071077503883\nIteration 7700 lower bound 4.722034252465465; gradient mag: 5.627923598935906\nIteration 7800 lower bound 4.6304661552408675; gradient mag: 5.396531979077218\nIteration 7900 lower bound 4.539356504132501; gradient mag: 5.2116672953127505\nIteration 8000 lower bound 4.45095546939241; gradient mag: 5.442647223211044\nIteration 8100 lower bound 4.3600620974975275; gradient mag: 5.203891398270185\nIteration 8200 lower bound 4.271237398872255; gradient mag: 5.050050070322315\nIteration 8300 lower bound 4.184440979576095; gradient mag: 5.246792589389047\nIteration 8400 lower bound 4.098659993997209; gradient mag: 5.578840619974407\nIteration 8500 lower bound 4.0122914943253445; gradient mag: 5.38143747376593\nIteration 8600 lower bound 3.9284516457036736; gradient mag: 5.346155288618987\nIteration 8700 lower bound 3.843939536519582; gradient mag: 9.992393975865559\nIteration 8800 lower bound 3.762354169166514; gradient mag: 9.79791476897377\nIteration 8900 lower bound 3.680935626235936; gradient mag: 9.23395114566231\nIteration 9000 lower bound 3.6026984128920043; gradient mag: 9.590713589033799\nIteration 9100 lower bound 3.522741405374154; gradient mag: 4.8315358324128885\nIteration 9200 lower bound 3.4454461281570974; gradient mag: 4.546978550730172\nIteration 9300 lower bound 3.3682333282315446; gradient mag: 8.077544206848168\nIteration 9400 lower bound 3.2931617717908868; gradient mag: 5.198762473040716\nIteration 9500 lower bound 3.215814184041519; gradient mag: 3.9061861443066372\nIteration 9600 lower bound 3.1406962333053876; gradient mag: 3.8992430901073822\nIteration 9700 lower bound 3.065866396957153; gradient mag: 6.9120899421115425\nIteration 9800 lower bound 2.990824541196763; gradient mag: 3.9861290594190173\nIteration 9900 lower bound 2.916690346335627; gradient mag: 6.789802894149524\nIteration 10000 lower bound 2.8425619783889506; gradient mag: 3.277143244727384\nIteration 10100 lower bound 2.7688158351972345; gradient mag: 2.6436406206321457\nIteration 10200 lower bound 2.6967620063896316; gradient mag: 1.7554182380350003\nIteration 10300 lower bound 2.626726890192881; gradient mag: 1.9079914587415372\nIteration 10400 lower bound 2.5588304285321404; gradient mag: 1.0932264174021213\nIteration 10500 lower bound 2.4941680185437414; gradient mag: 0.7730587380366593\nIteration 10600 lower bound 2.4333164901781315; gradient mag: 0.767066193236934\nIteration 10700 lower bound 2.376555349336709; gradient mag: 0.7553579421583788\nIteration 10800 lower bound 2.3238459422563444; gradient mag: 0.7228024683427984\nIteration 10900 lower bound 2.2753308343301666; gradient mag: 0.6845211470547401\nIteration 11000 lower bound 2.2312496509944224; gradient mag: 0.6413826587051727\nIteration 11100 lower bound 2.1918278044347352; gradient mag: 0.5943225261136371\nIteration 11200 lower bound 2.1572058127250573; gradient mag: 0.5444130874953347\nIteration 11300 lower bound 2.1273989093957475; gradient mag: 0.49279021686797564\nIteration 11400 lower bound 2.102281353226895; gradient mag: 0.4406015166810731\nIteration 11500 lower bound 2.0815910694099817; gradient mag: 0.38895956298853407\nIteration 11600 lower bound 2.064950099998059; gradient mag: 0.3388961083073893\nIteration 11700 lower bound 2.0518957709682164; gradient mag: 0.29131924913191776\nIteration 11800 lower bound 2.0419171399915657; gradient mag: 0.24697771788261189\nIteration 11900 lower bound 2.034491504029226; gradient mag: 0.206436219130497\nIteration 12000 lower bound 2.029116587353764; gradient mag: 0.1700642338625467\nIteration 12100 lower bound 2.025335369827164; gradient mag: 0.13803877718611637\nIteration 12200 lower bound 2.0227520966449344; gradient mag: 0.11035976785039378\nIteration 12300 lower bound 2.0210395323013945; gradient mag: 0.08687528646323558\nIteration 12400 lower bound 2.019938719988482; gradient mag: 0.06731321537522404\nIteration 12500 lower bound 2.0192532256688813; gradient mag: 0.051315577914184445\nIteration 12600 lower bound 2.0188400594620646; gradient mag: 0.03847223700586815\nIteration 12700 lower bound 2.018599266629263; gradient mag: 0.02835131706866659\nIteration 12800 lower bound 2.0184637229425713; gradient mag: 0.02052459535487594\nIteration 12900 lower bound 2.0183901201436676; gradient mag: 0.014586998208491776\nIteration 13000 lower bound 2.018351617807739; gradient mag: 0.010170104881023347\nIteration 13100 lower bound 2.0183322452743613; gradient mag: 0.00695013437505628\nIteration 13200 lower bound 2.0183228858875397; gradient mag: 0.004651251256489226\nIteration 13300 lower bound 2.0183185522950673; gradient mag: 0.0030451959052784556\nIteration 13400 lower bound 2.0183166332413283; gradient mag: 0.0019482655844909973\nIteration 13500 lower bound 2.0183158223137183; gradient mag: 0.0012165923762946223\nIteration 13600 lower bound 2.01831549612612; gradient mag: 0.0007405247328146874\nIteration 13700 lower bound 2.0183153715620206; gradient mag: 0.0004387539424643723\nIteration 13800 lower bound 2.01831532653006; gradient mag: 0.00025265818503618305\nIteration 13900 lower bound 2.0183153111653804; gradient mag: 0.00014117995468807326\nIteration 14000 lower bound 2.01831530623388; gradient mag: 7.641641264450355e-05\nIteration 14100 lower bound 2.0183153047500992; gradient mag: 3.999153140609237e-05\nIteration 14200 lower bound 2.0183153043331785; gradient mag: 2.0195504443660263e-05\nIteration 14300 lower bound 2.018315304224201; gradient mag: 9.820275317886484e-06\nIteration 14400 lower bound 2.018315304197827; gradient mag: 4.5876100911339915e-06\nIteration 14500 lower bound 2.0183153041919355; gradient mag: 2.0539213644144947e-06\nIteration 14600 lower bound 2.0183153041907285; gradient mag: 8.78979081776736e-07\nIteration 14700 lower bound 2.0183153041905064; gradient mag: 3.585482184541415e-07\nIteration 14800 lower bound 2.0183356471124654; gradient mag: 0.6225847787417013\nIteration 14900 lower bound 2.0183153050798386; gradient mag: 0.004103000324762999\n[[[ 0.          0.          0.          0.          0.\n    0.          9.46495004 10.17911794 10.89328584 11.60745374\n   12.32162164 13.03578954]\n  [ 0.          0.08053881  0.27984535  0.47915188  0.67845841\n    0.87776495  4.86389562  5.06320215  5.26250868  5.46181522\n    5.66112175  5.86042829]\n  [ 0.          0.          0.          0.          0.\n    0.          0.63281237  0.92101183  1.20921129  1.49741075\n    1.78561021  2.07380967]\n  [ 0.          0.          0.          0.          0.\n    0.         13.07263011 13.9874902  14.90235029 15.81721037\n   16.73207046 17.64693054]\n  [ 0.          0.          0.          0.          0.\n    0.          6.00257829  6.65421273  7.30584717  7.95748162\n    8.60911606  9.26075051]]]\n","output_type":"stream"},{"output_type":"error","ename":"ValueError","evalue":"shapes (1,5,12) and (12,5,1) not aligned: 12 (dim 2) != 5 (dim 1)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-110-a7f7806f8204>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_nlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-108-a2cfc0766ea5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     32\u001b[0m                                                                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                                                                 \u001b[0mx_test_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                                                                 samples=100)\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-108-a2cfc0766ea5>\u001b[0m in \u001b[0;36mget_bayes_lr_posterior\u001b[0;34m(self, prior_var, noise_var, x_matrix, y_matrix, x_test_matrix, samples)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mprior_precision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior_variance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mjoint_precision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprior_precision\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_matrix\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnoise_var\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mjoint_variance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoint_precision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mjoint_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoint_variance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnoise_var\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: shapes (1,5,12) and (12,5,1) not aligned: 12 (dim 2) != 5 (dim 1)"]}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00012-c1a75f06-df37-484a-b818-389925d2181a","output_cleared":false,"source_hash":"b623e53d"},"source":"","execution_count":null,"outputs":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"orig_nbformat":2,"deepnote_notebook_id":"bd9b06a9-900b-4258-b9d3-f28f55059097","deepnote_execution_queue":[]}}