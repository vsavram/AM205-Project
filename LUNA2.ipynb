{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00000-6ebfa645-e736-44d4-9e35-5a5dc6397b31",
    "deepnote_cell_type": "code",
    "execution_millis": 1296,
    "execution_start": 1606177914125,
    "output_cleared": false,
    "source_hash": "69f8c1eb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import standard libraries\n",
    "!pip install autograd\n",
    "from autograd import numpy as np\n",
    "from autograd import grad\n",
    "from autograd.misc.optimizers import adam, sgd\n",
    "from autograd import scipy as sp\n",
    "import autograd.numpy.random as npr\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "# import our libraries\n",
    "import bayes_helpers as bh\n",
    "from utils import generate_data\n",
    "from utils import run_toy_nn\n",
    "from feed_forward import Feedforward\n",
    "from nlm import NLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00001-5549cd5b-2550-4f55-9b68-f898233d5f31",
    "deepnote_cell_type": "code",
    "execution_millis": 2,
    "execution_start": 1606178026888,
    "output_cleared": false,
    "source_hash": "38f773d2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nlm import NLM\n",
    "\n",
    "class LUNA(NLM):\n",
    "    \"\"\"\n",
    "    Fits LUNA Model; inherits from NLM and overrides the objective function\n",
    "    \n",
    "    Model Assumptions\n",
    "     - Weights distributed normally\n",
    "     - Ys distributed normally\n",
    "\n",
    "     How to use:\n",
    "      - run train() to create: \n",
    "            a) the NN MLE weights, found in self.ff.weights \n",
    "            b) self.posterior samples, the distribution for the weights in the last layer of NLM\n",
    "     \n",
    "      - run predict() to get distribution of ys, given x test\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, architecture, grad_finite_diff=None):\n",
    "        prior_var = 1.\n",
    "        y_noise_var = 2.\n",
    "        random = np.random.RandomState(0)\n",
    "        #inherit from NLM, override objective func\n",
    "        super().__init__(prior_var, y_noise_var, architecture, random, self.make_objective) \n",
    "        self.D, self.D_out, self.H = self.ff.D, architecture['output_dim'], architecture['width']\n",
    "        \n",
    "        # override default finite difference method for cosine similarity calc (see cos_sim_sq function)\n",
    "        if grad_finite_diff:\n",
    "            self.grad_finite_diff = grad_finite_diff\n",
    "        else:\n",
    "            self.grad_finite_diff = self.default_grad_finite_diff\n",
    "\n",
    "    def get_aux_funcs(self, W):\n",
    "        # returns 1) list of aux func weights (a list-of-lists)\n",
    "        #     and 2) list of aux func biases (a list of numbers)\n",
    "        res_w, res_b = [], []\n",
    "        D, D_out, H = self.D, self.D_out, self.H\n",
    "        index = D - (D_out*H + D_out) - 1\n",
    "        for m in range(D_out):\n",
    "            w_m = W[0][index + H*m:index + H*(m+1)]\n",
    "            b_m = W[0][index + H*(m+1)]\n",
    "            index += 1\n",
    "            res_w.append(w_m)      \n",
    "            res_b.append(b_m)\n",
    "        return res_w, res_b\n",
    "\n",
    "    def default_grad_finite_diff(self, f, x):\n",
    "        # f is a vectorized function, x is a vector\n",
    "        dx = np.random.normal(0, 0.1) #see LUNA paper, appendix B.1\n",
    "        return (f(x + dx) - f(x))/dx\n",
    "\n",
    "    def similarity_score(self, W):\n",
    "        # returns sum of cosine similarity score over all pairwise combinations of aux functions\n",
    "        D, D_out, H = self.D, self.D_out, self.H      \n",
    "\n",
    "        def cos_sim_sq(fi, fj, x): \n",
    "            # returns 1 when fi parallel to fj\n",
    "            # returns 0 when fi perpendicular to fj\n",
    "            grad_i = self.grad_finite_diff(fi, x)\n",
    "            grad_j = self.grad_finite_diff(fj, x)\n",
    "            numerator = np.dot(grad_i, grad_j.T)**2\n",
    "            denominator = np.dot(grad_i, grad_i.T) * np.dot(grad_j, grad_j.T)\n",
    "            frac = numerator/denominator\n",
    "            return frac\n",
    "\n",
    "        # calculate square of cosine similarity for each pair of aux functions\n",
    "        score = 0\n",
    "        final_hidden_layer = self.ff.forward(W, x_train, final_layer_out=True)\n",
    "        aux_func_weights, aux_func_biases = self.get_aux_funcs(W)\n",
    "        for i in range(D_out):\n",
    "            w_i = aux_func_weights[i]\n",
    "            b_i = aux_func_biases[i]\n",
    "            f_i = lambda x : np.matmul(w_i, x) + b_i#applying aux weights w_i to last hidden layer\n",
    "            for j in range(i + 1, D_out):\n",
    "                w_j = aux_func_weights[j]\n",
    "                b_j = aux_func_biases[j]\n",
    "                f_j = lambda x : np.matmul(w_j, x) + b_j#applying aux weights w_j to last hidden layer\n",
    "                score += cos_sim_sq(f_i, f_j, final_hidden_layer)\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def mean_mean_sq_error(self, W):\n",
    "        # returns mean of the mse across all aux regressors\n",
    "        D, D_out, H = self.D, self.D_out, self.H\n",
    "        aux_outputs = self.ff.forward(W, x_train) #shape = (1,10,12)\n",
    "        Y = np.tile(y_train, D_out).reshape(1, D_out, y_train.shape[1])\n",
    "\n",
    "        # calculate squared error for each aux regressor, take mean\n",
    "        mean_mse = np.mean(np.linalg.norm(Y - aux_outputs, axis=1)**2)\n",
    "\n",
    "        return mean_mse\n",
    "\n",
    "    # for LUNA, this needs to use aux functions\n",
    "    def make_objective(self, x_train, y_train, reg_param):\n",
    "\n",
    "        def objective(W, t):\n",
    "            # L_luna(model) = L_fit(model) - L_similarity(model)\n",
    "            reg_param = 0.1 #eventually this will become an input\n",
    "            lambda_ = 0.1 #eventually this will become an input\n",
    "            L_sim = lambda_*self.similarity_score(W)\n",
    "\n",
    "            regularization_penalty = reg_param*np.linalg.norm(W)**2\n",
    "            mean_mse = self.mean_mean_sq_error(W)\n",
    "            L_fit = mean_mse - regularization_penalty\n",
    "\n",
    "            return L_fit - L_sim\n",
    "\n",
    "        return objective, grad(objective)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00002-bf8423c9-ba2e-4ce7-b6e1-606efea2a4cf",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Define Hyperameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00002-a9356595-b2ac-49d6-8bb3-6bd9dd3ea019",
    "deepnote_cell_type": "code",
    "execution_millis": 1,
    "execution_start": 1606178029561,
    "output_cleared": false,
    "source_hash": "8f9edc5a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "###relu activation\n",
    "activation_fn_type = 'relu'\n",
    "activation_fn = lambda x: np.maximum(np.zeros(x.shape), x)\n",
    "\n",
    "\n",
    "###neural network model design choices\n",
    "width = 7\n",
    "hidden_layers = 2\n",
    "input_dim = 1\n",
    "output_dim = 10 #number of auxiliary functions\n",
    "\n",
    "architecture = {'width': width,\n",
    "               'hidden_layers': hidden_layers,\n",
    "               'input_dim': input_dim,\n",
    "               'output_dim': output_dim,\n",
    "               'activation_fn_type': 'relu',\n",
    "               'activation_fn_params': 'rate=1',\n",
    "               'activation_fn': activation_fn}\n",
    "\n",
    "luna = LUNA(architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00003-b88d04db-ce45-454b-95cb-b3d3193f7c6d",
    "deepnote_cell_type": "code",
    "execution_millis": 9828,
    "execution_start": 1606178030388,
    "output_cleared": false,
    "source_hash": "1731533f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"HW8_data.csv\")\n",
    "x_train = np.array(df[\"x\"])\n",
    "y_train = np.array(df[\"y\"])\n",
    "x_test = np.linspace(x_train.min()-1,x_train.max()+1,200)\n",
    "\n",
    "x_train = x_train.reshape((1, -1))\n",
    "y_train = y_train.reshape((1, -1))\n",
    "x_test = x_test.reshape((1, -1))\n",
    "\n",
    "params = {'step_size':1e-3, \n",
    "          'max_iteration':100, \n",
    "          'random_restarts':1,\n",
    "          'optimizer':'adam'}\n",
    "\n",
    "luna.train(x_train, y_train, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00005-f1227df7-ab68-48c9-b8b3-214722f6c386",
    "deepnote_cell_type": "code",
    "execution_millis": 299,
    "execution_start": 1606178043110,
    "output_cleared": false,
    "source_hash": "52d1573e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predict on the test x-values\n",
    "posterior_predictions, posterior_predictive_samples = luna.predict(x_test)\n",
    "bh.viz_pp_samples(x_train,y_train,x_test.flatten(),posterior_predictive_samples,'La Luna!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00006-8a3b9550-34a7-40da-8813-761bccb57e75",
    "deepnote_cell_type": "code",
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "3f0571a8-32c3-4550-8344-a31fd0bea951",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
