{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00000-244a20a3-5850-49a8-b3c4-47c8dd90e3f4",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "# 205 Scratch\n",
    "\n",
    "Paper: *Learned Uncertainty-Aware (LUNA) Bases for Bayesian Regression using Multi-Headed Auxiliary Networks*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00001-82b15dd3-3681-4233-b273-015b39362a48",
    "execution_millis": 4181,
    "execution_start": 1605367204124,
    "output_cleared": false,
    "source_hash": "de8f455b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from autograd import numpy as np\n",
    "from autograd import grad\n",
    "from autograd.misc.optimizers import adam, sgd\n",
    "from autograd import scipy as sp\n",
    "import autograd.numpy.random as npr\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00002-133afcba-b5c2-49d5-828a-458e46e60d99",
    "execution_millis": 0,
    "execution_start": 1605367309817,
    "output_cleared": false,
    "source_hash": "4b51ed9d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Feedforward:\n",
    "    def __init__(self, architecture, random=None, weights=None):\n",
    "        self.params = {'H': architecture['width'],\n",
    "                       'L': architecture['hidden_layers'],\n",
    "                       'D_in': architecture['input_dim'],\n",
    "                       'D_out': architecture['output_dim'],\n",
    "                       'activation_type': architecture['activation_fn_type'],\n",
    "                       'activation_params': architecture['activation_fn_params']}\n",
    "\n",
    "        self.D = (  (architecture['input_dim'] * architecture['width'] + architecture['width'])\n",
    "                  + (architecture['output_dim'] * architecture['width'] + architecture['output_dim'])\n",
    "                  + (architecture['hidden_layers'] - 1) * (architecture['width']**2 + architecture['width'])\n",
    "                 )\n",
    "\n",
    "        if random is not None:\n",
    "            self.random = random\n",
    "        else:\n",
    "            self.random = np.random.RandomState(0)\n",
    "\n",
    "        self.h = architecture['activation_fn']\n",
    "\n",
    "        if weights is None:\n",
    "            self.weights = self.random.normal(0, 1, size=(1, self.D))\n",
    "        else:\n",
    "            self.weights = weights\n",
    "\n",
    "        self.objective_trace = np.empty((1, 1))\n",
    "        self.weight_trace = np.empty((1, self.D))\n",
    "\n",
    "\n",
    "    def forward(self, weights, x, final_layer_out=False):\n",
    "        ''' Forward pass given weights and input '''\n",
    "        H = self.params['H']\n",
    "        D_in = self.params['D_in']\n",
    "        D_out = self.params['D_out']\n",
    "\n",
    "        assert weights.shape[1] == self.D\n",
    "\n",
    "        if len(x.shape) == 2:\n",
    "            assert x.shape[0] == D_in\n",
    "            x = x.reshape((1, D_in, -1))\n",
    "        else:\n",
    "            assert x.shape[1] == D_in\n",
    "\n",
    "        weights = weights.T\n",
    "\n",
    "        #input to first hidden layer\n",
    "        W = weights[:H * D_in].T.reshape((-1, H, D_in))\n",
    "        b = weights[H * D_in:H * D_in + H].T.reshape((-1, H, 1))\n",
    "        input = self.h(np.matmul(W, x) + b)\n",
    "        index = H * D_in + H\n",
    "\n",
    "        assert input.shape[1] == H\n",
    "\n",
    "        #additional hidden layers\n",
    "        for _ in range(self.params['L'] - 1):\n",
    "            before = index\n",
    "            W = weights[index:index + H * H].T.reshape((-1, H, H))\n",
    "            index += H * H\n",
    "            b = weights[index:index + H].T.reshape((-1, H, 1))\n",
    "            index += H\n",
    "            output = np.matmul(W, input) + b\n",
    "            input = self.h(output)\n",
    "\n",
    "            assert input.shape[1] == H\n",
    "\n",
    "        #output layer #THIS IS JUST FOR FINAL LAYER, INPUT STORES VALUES OF FINAL ALYER\n",
    "        W = weights[index:index + H * D_out].T.reshape((-1, D_out, H))\n",
    "        b = weights[index + H * D_out:].T.reshape((-1, D_out, 1))\n",
    "        output = np.matmul(W, input) + b\n",
    "        assert output.shape[1] == self.params['D_out']\n",
    "\n",
    "        #finallayer\n",
    "        final_layer = np.array(input, copy=True) #autograd doesn't like np.copy\n",
    "\n",
    "        if final_layer_out:\n",
    "            return final_layer\n",
    "        else:\n",
    "            return output\n",
    "    \n",
    "    def make_objective(self, x_train, y_train, reg_param):\n",
    "\n",
    "        def objective(W, t):\n",
    "            squared_error = np.linalg.norm(y_train - self.forward(W, x_train), axis=1)**2\n",
    "            if reg_param is None:\n",
    "                sum_error = np.sum(squared_error)\n",
    "                return sum_error\n",
    "            else:\n",
    "                mean_error = np.mean(squared_error) + reg_param * np.linalg.norm(W)\n",
    "                return mean_error\n",
    "\n",
    "        return objective, grad(objective)\n",
    "    \n",
    "    \n",
    "    def fit(self, x_train, y_train, params, reg_param=None,):\n",
    "\n",
    "        assert x_train.shape[0] == self.params['D_in']\n",
    "        assert y_train.shape[0] == self.params['D_out']\n",
    "\n",
    "        ### make objective function for training\n",
    "        self.objective, self.gradient = self.make_objective(x_train, y_train, reg_param)\n",
    "\n",
    "        ### set up optimization\n",
    "        step_size = 0.01\n",
    "        max_iteration = 5000\n",
    "        check_point = 100\n",
    "        weights_init = self.weights.reshape((1, -1))\n",
    "        mass = None\n",
    "        optimizer = 'adam' # DEFAULT. CHANGE IN PARAMS\n",
    "        opt_gradient = self.gradient # DEFAULT. CHANGE IN PARAMS\n",
    "        random_restarts = 5\n",
    "\n",
    "        if 'step_size' in params.keys():\n",
    "            step_size = params['step_size']\n",
    "        if 'max_iteration' in params.keys():\n",
    "            max_iteration = params['max_iteration']\n",
    "        if 'check_point' in params.keys():\n",
    "            self.check_point = params['check_point']\n",
    "        if 'init' in params.keys():\n",
    "            weights_init = params['init']\n",
    "        if 'call_back' in params.keys():\n",
    "            call_back = params['call_back']\n",
    "        if 'mass' in params.keys():\n",
    "            mass = params['mass']\n",
    "        if 'optimizer' in params.keys():\n",
    "            optimizer = params['optimizer']\n",
    "        if  'opt_gradient' in params.keys():\n",
    "            gradient = params['opt_gradient']\n",
    "        if 'random_restarts' in params.keys():\n",
    "            random_restarts = params['random_restarts']\n",
    "\n",
    "        def call_back(weights, iteration, g):\n",
    "            ''' Actions per optimization step '''\n",
    "            objective = self.objective(weights, iteration)\n",
    "            self.objective_trace = np.vstack((self.objective_trace, objective))\n",
    "            self.weight_trace = np.vstack((self.weight_trace, weights))\n",
    "            if iteration % check_point == 0:\n",
    "                print(\"Iteration {} lower bound {}; gradient mag: {}\".format(iteration, objective, np.linalg.norm(self.gradient(weights, iteration))))\n",
    "\n",
    "        ### train with random restarts\n",
    "        optimal_obj = 1e16\n",
    "        optimal_weights = self.weights\n",
    "        \n",
    "        # AM 205: THIS IS THE ONLY PLACE YOU WILL NEED TO MODIFY CODE\n",
    "        for i in range(random_restarts):\n",
    "\n",
    "            \n",
    "            if optimizer == 'adam':\n",
    "                adam(opt_gradient, weights_init, step_size=step_size, num_iters=max_iteration, callback=call_back)\n",
    "            \n",
    "            local_opt = np.min(self.objective_trace[-100:])\n",
    "            if local_opt < optimal_obj:\n",
    "                opt_index = np.argmin(self.objective_trace[-100:])\n",
    "                self.weights = self.weight_trace[-100:][opt_index].reshape((1, -1))\n",
    "            weights_init = self.random.normal(0, 1, size=(1, self.D))\n",
    "\n",
    "        self.objective_trace = self.objective_trace[1:]\n",
    "        self.weight_trace = self.weight_trace[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00002-da662230-83a9-4a83-a071-abb21b920110",
    "execution_millis": 2,
    "execution_start": 1605367310838,
    "output_cleared": false,
    "source_hash": "2d09fe3d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_csv('HW8_data.csv')\n",
    "x_train = data['x'].values.reshape((1, -1))\n",
    "y_train = data['y'].values.reshape((1, -1))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00004-6632e33c-4eea-427c-9d17-87c54912c224",
    "execution_millis": 2,
    "execution_start": 1605367311984,
    "output_cleared": false,
    "source_hash": "aeae959c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "###relu activation\n",
    "activation_fn_type = 'relu'\n",
    "activation_fn = lambda x: np.maximum(np.zeros(x.shape), x)\n",
    "\n",
    "\n",
    "###neural network model design choices\n",
    "width = 5\n",
    "hidden_layers = 1\n",
    "input_dim = 1\n",
    "output_dim = 1\n",
    "\n",
    "architecture = {'width': width,\n",
    "               'hidden_layers': hidden_layers,\n",
    "               'input_dim': input_dim,\n",
    "               'output_dim': output_dim,\n",
    "               'activation_fn_type': 'relu',\n",
    "               'activation_fn_params': 'rate=1',\n",
    "               'activation_fn': activation_fn}\n",
    "\n",
    "#set random state to make the experiments replicable\n",
    "rand_state = 0\n",
    "random = np.random.RandomState(rand_state)\n",
    "\n",
    "#instantiate a Feedforward neural network object\n",
    "nn = Feedforward(architecture, random=random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00005-9ecfb438-c35a-48c3-b29e-107f9e825364",
    "execution_millis": 19351,
    "execution_start": 1605367314015,
    "output_cleared": false,
    "source_hash": "f1861af",
    "tags": []
   },
   "outputs": [],
   "source": [
    "###define design choices in gradient descent\n",
    "params = {'step_size':1e-3, \n",
    "          'max_iteration':15000, \n",
    "          'random_restarts':1,\n",
    "          'optimizer':'adam'}\n",
    "\n",
    "#fit my neural network to minimize MSE on the given data\n",
    "nn.fit(x_train, y_train, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00006-636ff1b9-fa81-43a9-aeaa-6ca733538a52",
    "execution_millis": 129,
    "execution_start": 1605302323264,
    "output_cleared": false,
    "source_hash": "febdf14f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#test x-values\n",
    "x_test = np.linspace(-8, 8, 100).reshape((1, -1))\n",
    "print(nn.weights.shape, x_test.shape)\n",
    "#predict on the test x-values\n",
    "y_test_pred = nn.forward(nn.weights, x_test)\n",
    "#visualize the function learned by the neural network\n",
    "plt.scatter(x_train.flatten(), y_train.flatten(), color='black', label='data')\n",
    "plt.plot(x_test.flatten(), y_test_pred.flatten(), color='red', label='learned neural network function')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00000-2aed2219-ce40-41ba-9030-c82365f32b30",
    "execution_millis": 1,
    "execution_start": 1605303617152,
    "output_cleared": false,
    "source_hash": "f3158ead",
    "tags": []
   },
   "outputs": [],
   "source": [
    "##\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "class NLM():\n",
    "\n",
    "    def __init__(self,X,Y,prior_var,y_var,architecture, random_state):\n",
    "\n",
    "        self.ff = Feedforward(architecture, random = random_state) #CHANGE\n",
    "        \n",
    "        #super().__init__(architecture)\n",
    "        self.X = X # X training\n",
    "        self.Y = Y # Y training data\n",
    "        \n",
    "        self.prior_var = prior_var\n",
    "        self.y_var = y_var\n",
    "        self.w_prior_pdf = lambda w : multivariate_normal(mean = np.zeros(shape = w.shape), cov = prior_variance*np.eye(w.shape[1])).pdf(w)\n",
    "        self.w_prior_sampler = lambda : np.random.default_rng().multivariate_normal(mean = np.zeros(shape = w.shape), cov = prior_variance*np.eye(w.shape[1]))\n",
    "\n",
    "    def train(self):\n",
    "        \n",
    "        # Fit Weights\n",
    "        self.ff.fit(self.X, self.Y, params)\n",
    "\n",
    "        # Transform X with Feature Map for Bayes Reg\n",
    "        fm_x_matrix = self.ff.forward(self.ff.weights, self.X, final_layer_out=True)\n",
    "        \n",
    "        print(fm_x_matrix)\n",
    "        # Conduct Bayes Reg on Final Layer \n",
    "        posterior_samples = self.get_bayes_lr_posterior(self.prior_var,\n",
    "                                                                self.y_var,\n",
    "                                                                fm_x_matrix.T, #fm_x_matrix, \n",
    "                                                                self.Y.T, \n",
    "                                                                x_test_matrix = None, \n",
    "                                                                samples=100)\n",
    "\n",
    "\n",
    "        print(\"DONE\")\n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "    def get_bayes_lr_posterior(self, prior_var, noise_var, x_matrix, y_matrix, x_test_matrix=None, samples=100):\n",
    "        '''Function to generate posterior predictive samples for Bayesian linear regression model'''\n",
    "        prior_variance = np.diag(prior_var * np.ones(x_matrix.shape[1])) # make it 2 D\n",
    "        prior_precision = np.linalg.inv(prior_variance)\n",
    "\n",
    "        joint_precision = prior_precision + x_matrix.T.dot(x_matrix) / noise_var \n",
    "        joint_variance = np.linalg.inv(joint_precision)\n",
    "        joint_mean = joint_variance.dot(x_matrix.T.dot(y_matrix)) / noise_var\n",
    "\n",
    "        #sampling 100 points from the posterior\n",
    "        posterior_samples = np.random.multivariate_normal(joint_mean.flatten(), joint_variance, size=samples)\n",
    "\n",
    "        #take posterior predictive samples\n",
    "        if x_test_matrix != None:\n",
    "            posterior_predictions = np.dot(posterior_samples, x_test_matrix.T) \n",
    "            posterior_predictive_samples = posterior_predictions[np.newaxis, :, :] + np.random.normal(0, noise_var**0.5, size=(100, posterior_predictions.shape[0], posterior_predictions.shape[1]))\n",
    "            posterior_predictive_samples = posterior_predictive_samples.reshape((100 * posterior_predictions.shape[0], posterior_predictions.shape[1]))\n",
    "            return posterior_predictions, posterior_predictive_samples\n",
    "        else:\n",
    "            return posterior_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00000-f0eaac6f-fe09-47ff-8d9b-6b57b7ca5d3e",
    "execution_millis": 3,
    "execution_start": 1605303618273,
    "output_cleared": false,
    "source_hash": "d9f4c07",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "prior_var = 1.0\n",
    "y_var = 2.0\n",
    "test_nlm = NLM(x_train, y_train, prior_var,y_var, architecture, random_state = np.random.RandomState(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00005-3b5e8825-04a1-4a79-80e0-44a0bbabf7d5",
    "execution_millis": 19859,
    "execution_start": 1605303618618,
    "output_cleared": false,
    "source_hash": "476b857b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_nlm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00012-c1a75f06-df37-484a-b818-389925d2181a",
    "output_cleared": false,
    "source_hash": "b623e53d",
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "bd9b06a9-900b-4258-b9d3-f28f55059097",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
